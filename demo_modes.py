"""
Demo execution modes and runners for different testing scenarios.
"""

import asyncio
import json
import random
import time
from datetime import datetime
from typing import Dict, List, Any, Optional

from demo_output import DemoOutput, DemoMode


class DemoMode:
    """Enumeration of supported demo modes."""
    INTERACTIVE = "interactive"
    AUTOMATED = "automated"
    BENCHMARK = "benchmark"
    VISUALIZATION = "visualization"


class DemoRunner:
    """Handles different demo execution modes and scenarios."""
    
    def __init__(self):
        self.output = DemoOutput()
    
    async def run_stress_test(self, agents: Dict[str, Any], memory: Any, duration_seconds: int = 60):
        """Run stress test with continuous load for specified duration."""
        self.output.print_section("ğŸ”¥ å‹åŠ›æµ‹è¯•æ¨¡å¼")
        
        # Stress test questions pool
        questions_pool = [
            "å¦‚ä½•ä¼˜åŒ– Python åˆ—è¡¨æ“ä½œï¼Ÿ",
            "Milvus å‘é‡æœç´¢çš„æœ€ä½³é…ç½®ï¼Ÿ",
            "Docker å®¹å™¨ä¼˜åŒ–æŠ€å·§ï¼Ÿ",
            "å¼‚æ­¥ç¼–ç¨‹çš„æ€§èƒ½æå‡ï¼Ÿ",
            "æ•°æ®åº“ç´¢å¼•ä¼˜åŒ–ç­–ç•¥ï¼Ÿ",
            "å¾®æœåŠ¡æ¶æ„è®¾è®¡åŸåˆ™ï¼Ÿ",
            "æœºå™¨å­¦ä¹ æ¨¡å‹éƒ¨ç½²ï¼Ÿ",
            "ç¼“å­˜ç­–ç•¥æœ€ä½³å®è·µï¼Ÿ"
        ]
        
        start_time = time.time()
        end_time = start_time + duration_seconds
        question_count = 0
        success_count = 0
        error_count = 0
        
        self.output.print_info(f"ğŸš€ å¼€å§‹ {duration_seconds} ç§’å‹åŠ›æµ‹è¯•...")
        
        async def send_question():
            nonlocal question_count, success_count, error_count
            
            while time.time() < end_time:
                try:
                    question = random.choice(questions_pool)
                    question_count += 1
                    
                    # Simulate processing through coordinator
                    coordinator = agents["coordinator"]
                    message = {
                        "content": {"text": question},
                        "id": f"stress_{question_count:06d}",
                        "tenant_id": "stress_test",
                        "timestamp": datetime.now().isoformat()
                    }
                    
                    response = await coordinator.handle_message(message)
                    
                    if response and response.content:
                        success_count += 1
                    else:
                        error_count += 1
                        
                except Exception as e:
                    error_count += 1
                
                # Small random delay to simulate real usage
                await asyncio.sleep(random.uniform(0.1, 0.5))
        
        # Start multiple concurrent workers
        workers = [asyncio.create_task(send_question()) for _ in range(3)]
        
        # Monitor progress
        while time.time() < end_time:
            await asyncio.sleep(5)
            elapsed = time.time() - start_time
            remaining = end_time - time.time()
            rate = question_count / elapsed if elapsed > 0 else 0
            
            self.output.print_info(
                f"â±ï¸  å·²è¿è¡Œ: {elapsed:.1f}s | å‰©ä½™: {remaining:.1f}s | "
                f"é—®é¢˜æ•°: {question_count} | æˆåŠŸ: {success_count} | å¤±è´¥: {error_count} | "
                f"é€Ÿç‡: {rate:.2f} q/s"
            )
        
        # Wait for all workers to finish
        await asyncio.gather(*workers, return_exceptions=True)
        
        # Final statistics
        total_time = time.time() - start_time
        final_rate = question_count / total_time if total_time > 0 else 0
        success_rate = (success_count / question_count * 100) if question_count > 0 else 0
        
        stats = f"""
ğŸ“Š å‹åŠ›æµ‹è¯•ç»“æœ:
æ€»è€—æ—¶: {total_time:.2f} ç§’
æ€»é—®é¢˜æ•°: {question_count}
æˆåŠŸå¤„ç†: {success_count}
å¤„ç†å¤±è´¥: {error_count}
æˆåŠŸç‡: {success_rate:.1f}%
å¹³å‡é€Ÿç‡: {final_rate:.2f} é—®é¢˜/ç§’
        """
        self.output.print_info(stats)
    
    async def run_knowledge_accumulation_test(self, agents: Dict[str, Any], memory: Any):
        """Test knowledge accumulation and retrieval over multiple sessions."""
        self.output.print_section("ğŸ§  çŸ¥è¯†ç§¯ç´¯æµ‹è¯•")
        
        # Related questions that should benefit from accumulated knowledge
        question_sets = [
            {
                "topic": "Pythonæ€§èƒ½ä¼˜åŒ–",
                "questions": [
                    "å¦‚ä½•ä¼˜åŒ– Python å¾ªç¯æ€§èƒ½ï¼Ÿ",
                    "Python åˆ—è¡¨æ¨å¯¼å¼ä¸ºä»€ä¹ˆæ¯” for å¾ªç¯å¿«ï¼Ÿ",
                    "å¦‚ä½•ä½¿ç”¨ NumPy æå‡ Python è®¡ç®—æ€§èƒ½ï¼Ÿ",
                    "Python å¤šçº¿ç¨‹ vs å¤šè¿›ç¨‹çš„æ€§èƒ½å¯¹æ¯”ï¼Ÿ"
                ]
            },
            {
                "topic": "Milvuså‘é‡æ•°æ®åº“",
                "questions": [
                    "Milvus å¦‚ä½•å¤„ç†å¤§è§„æ¨¡å‘é‡æ•°æ®ï¼Ÿ",
                    "Milvus çš„ç´¢å¼•ç±»å‹é€‰æ‹©å»ºè®®ï¼Ÿ",
                    "å¦‚ä½•ä¼˜åŒ– Milvus æŸ¥è¯¢æ€§èƒ½ï¼Ÿ",
                    "Milvus é›†ç¾¤éƒ¨ç½²çš„æœ€ä½³å®è·µï¼Ÿ"
                ]
            }
        ]
        
        for topic_set in question_sets:
            topic = topic_set["topic"]
            questions = topic_set["questions"]
            
            self.output.print_info(f"\nğŸ“š æµ‹è¯•ä¸»é¢˜: {topic}")
            
            # Process questions sequentially to build knowledge
            for i, question in enumerate(questions, 1):
                self.output.print_info(f"\n[{i}/{len(questions)}] {question}")
                
                try:
                    coordinator = agents["coordinator"]
                    message = {
                        "content": {"text": question},
                        "id": f"knowledge_{topic}_{i}",
                        "tenant_id": "knowledge_test",
                        "timestamp": datetime.now().isoformat()
                    }
                    
                    start_time = time.time()
                    response = await coordinator.handle_message(message)
                    processing_time = time.time() - start_time
                    
                    if response and response.content:
                        # Show if similar knowledge was found
                        metadata = response.metadata or {}
                        similar_count = metadata.get("similar_documents_count", 0)
                        
                        self.output.print_success(
                            f"âœ… å¤„ç†å®Œæˆ ({processing_time:.2f}s) | "
                            f"æ‰¾åˆ°ç›¸å…³çŸ¥è¯†: {similar_count} æ¡"
                        )
                        
                        # Show brief preview of answer
                        answer_preview = response.content[:100] + "..." if len(response.content) > 100 else response.content
                        self.output.print_info(f"ğŸ’¡ ç­”æ¡ˆé¢„è§ˆ: {answer_preview}")
                    else:
                        self.output.print_error("âŒ å¤„ç†å¤±è´¥")
                        
                except Exception as e:
                    self.output.print_error(f"âŒ é”™è¯¯: {e}")
                
                await asyncio.sleep(1)  # Small delay between questions
    
    async def run_error_recovery_test(self, agents: Dict[str, Any], memory: Any):
        """Test system behavior under error conditions and recovery."""
        self.output.print_section("ğŸ›¡ï¸  é”™è¯¯æ¢å¤æµ‹è¯•")
        
        # Test scenarios that might cause errors
        error_scenarios = [
            {
                "name": "ç©ºé—®é¢˜æµ‹è¯•",
                "question": "",
                "expected_behavior": "åº”è¯¥ä¼˜é›…å¤„ç†ç©ºè¾“å…¥"
            },
            {
                "name": "è¶…é•¿é—®é¢˜æµ‹è¯•", 
                "question": "å¦‚ä½•" * 1000,  # Very long question
                "expected_behavior": "åº”è¯¥å¤„ç†é•¿æ–‡æœ¬æˆ–ç»™å‡ºåˆç†é™åˆ¶"
            },
            {
                "name": "ç‰¹æ®Šå­—ç¬¦æµ‹è¯•",
                "question": "å¦‚ä½•å¤„ç† ğŸš€ emoji å’Œ ç‰¹æ®Šå­—ç¬¦ @#$%^&*()?",
                "expected_behavior": "åº”è¯¥æ­£ç¡®å¤„ç†ç‰¹æ®Šå­—ç¬¦"
            },
            {
                "name": "æ··åˆè¯­è¨€æµ‹è¯•",
                "question": "How to optimize Python æ€§èƒ½ for ä¸­æ–‡ users?",
                "expected_behavior": "åº”è¯¥å¤„ç†æ··åˆè¯­è¨€è¾“å…¥"
            }
        ]
        
        for scenario in error_scenarios:
            self.output.print_info(f"\nğŸ§ª {scenario['name']}")
            self.output.print_info(f"é¢„æœŸ: {scenario['expected_behavior']}")
            
            try:
                coordinator = agents["coordinator"]
                message = {
                    "content": {"text": scenario["question"]},
                    "id": f"error_test_{scenario['name']}",
                    "tenant_id": "error_test",
                    "timestamp": datetime.now().isoformat()
                }
                
                start_time = time.time()
                response = await coordinator.handle_message(message)
                processing_time = time.time() - start_time
                
                if response and response.content:
                    self.output.print_success(f"âœ… æ­£å¸¸å¤„ç† ({processing_time:.2f}s)")
                    
                    # Check if response is reasonable
                    if len(response.content) > 10:
                        self.output.print_info("ğŸ’¡ è¿”å›äº†æœ‰æ„ä¹‰çš„å›ç­”")
                    else:
                        self.output.print_warning("âš ï¸  å›ç­”å¯èƒ½è¿‡çŸ­")
                else:
                    self.output.print_warning("âš ï¸  æ²¡æœ‰è¿”å›å›ç­”ï¼Œä½†ç³»ç»Ÿæ²¡æœ‰å´©æºƒ")
                    
            except Exception as e:
                # Check if it's a handled error or system crash
                error_msg = str(e).lower()
                if any(keyword in error_msg for keyword in ["timeout", "rate", "limit"]):
                    self.output.print_warning(f"âš ï¸  å¯é¢„æœŸçš„é”™è¯¯: {e}")
                else:
                    self.output.print_error(f"âŒ æ„å¤–é”™è¯¯: {e}")
            
            await asyncio.sleep(0.5)
    
    async def run_collaboration_test(self, agents: Dict[str, Any], memory: Any):
        """Test multi-agent collaboration scenarios."""
        self.output.print_section("ğŸ¤ åä½œæµ‹è¯•")
        
        # Questions that should require multiple experts
        collaboration_questions = [
            {
                "question": "å¦‚ä½•æ„å»ºä¸€ä¸ªé«˜æ€§èƒ½çš„ Python + Milvus å‘é‡æœç´¢ç³»ç»Ÿå¹¶è¿›è¡Œå®¹å™¨åŒ–éƒ¨ç½²ï¼Ÿ",
                "expected_experts": ["python_expert", "milvus_expert", "devops_expert"],
                "complexity": "high"
            },
            {
                "question": "ä½¿ç”¨ Python å¼€å‘ Milvus æ•°æ®è¿ç§»å·¥å…·çš„ DevOps æœ€ä½³å®è·µï¼Ÿ",
                "expected_experts": ["python_expert", "milvus_expert", "devops_expert"],
                "complexity": "medium"
            },
            {
                "question": "Python å¼‚æ­¥ç¼–ç¨‹åœ¨ Milvus å®¢æˆ·ç«¯ä¸­çš„åº”ç”¨å’Œæ€§èƒ½ç›‘æ§ï¼Ÿ",
                "expected_experts": ["python_expert", "milvus_expert"],
                "complexity": "medium"
            }
        ]
        
        for i, test_case in enumerate(collaboration_questions, 1):
            question = test_case["question"]
            expected_experts = test_case["expected_experts"]
            complexity = test_case["complexity"]
            
            self.output.print_info(f"\n[{i}] åä½œæµ‹è¯• - å¤æ‚åº¦: {complexity}")
            self.output.print_info(f"é—®é¢˜: {question}")
            self.output.print_info(f"é¢„æœŸå‚ä¸ä¸“å®¶: {', '.join(expected_experts)}")
            
            try:
                coordinator = agents["coordinator"]
                message = {
                    "content": {"text": question},
                    "id": f"collab_{i}",
                    "tenant_id": "collaboration_test",
                    "timestamp": datetime.now().isoformat()
                }
                
                start_time = time.time()
                response = await coordinator.handle_message(message)
                processing_time = time.time() - start_time
                
                if response and response.content:
                    metadata = response.metadata or {}
                    
                    # Check collaboration metadata
                    participating_agents = metadata.get("participating_agents", [])
                    expert_responses = metadata.get("expert_responses", {})
                    
                    self.output.print_success(f"âœ… åä½œå®Œæˆ ({processing_time:.2f}s)")
                    self.output.print_info(f"ğŸ¤ å‚ä¸æ™ºèƒ½ä½“: {participating_agents}")
                    
                    # Show expert responses summary
                    for expert, expert_data in expert_responses.items():
                        if isinstance(expert_data, dict) and expert_data.get("response"):
                            response_preview = expert_data["response"][:50] + "..."
                            self.output.print_info(f"  ğŸ“ {expert}: {response_preview}")
                    
                    # Show final answer preview
                    answer_preview = response.content[:100] + "..." if len(response.content) > 100 else response.content
                    self.output.print_info(f"ğŸ’¡ ç»¼åˆç­”æ¡ˆ: {answer_preview}")
                    
                else:
                    self.output.print_error("âŒ åä½œå¤±è´¥")
                    
            except Exception as e:
                self.output.print_error(f"âŒ åä½œæµ‹è¯•é”™è¯¯: {e}")
            
            await asyncio.sleep(2)  # Longer delay for complex collaboration tests