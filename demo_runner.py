#!/usr/bin/env python3
"""
End-to-end Multi-Agent Brain DEMO

This script demonstrates the complete workflow of the multi-agent system:
1. User asks a question
2. Coordinator analyzes and routes to experts
3. Expert agents process in parallel
4. Coordinator synthesizes results
5. Knowledge is stored in SharedMemory
6. Final answer is presented with metrics
"""

import asyncio
import json
import os
import sys
import time
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Any

import click
from loguru import logger

# Add project root to path
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from agents.coordination import CoordinationAgent
from agents.python_expert import PythonExpertAgent
from agents.milvus_expert import MilvusExpertAgent
from agents.devops_expert import DevOpsExpertAgent
from agents.shared_memory import SharedMemory
from utils import get_agent_config, OpenAIClientWrapper
from demo_setup import check_environment, DemoEnvironmentError
from demo_output import DemoOutput, DemoMode
from demo_modes import DemoRunner


class MultiAgentDemo:
    """Main DEMO orchestrator for the multi-agent brain system."""

    def __init__(self, mode: str = "interactive", config_file: str = "config.yaml"):
        """Initialize the DEMO with specified mode and configuration."""
        self.mode = DemoMode(mode)
        self.config_file = config_file
        self.agents = {}
        self.memory = None
        self.output = DemoOutput()
        self.runner = DemoRunner()
        self.question_count = 0

    async def setup_agents(self) -> Dict[str, Any]:
        """Initialize all agents and shared memory."""
        self.output.print_section("ğŸš€ å¯åŠ¨æ™ºèƒ½ä½“ç½‘ç»œ")
        
        try:
            # Initialize SharedMemory first
            self.memory = SharedMemory()
            self.output.print_success("âœ… SharedMemory åˆå§‹åŒ–å®Œæˆ")

            # Initialize all expert agents
            self.agents["coordinator"] = CoordinationAgent()
            self.output.print_success("âœ… CoordinatorAgent åˆå§‹åŒ–å®Œæˆ")

            self.agents["python_expert"] = PythonExpertAgent()
            self.output.print_success("âœ… PythonExpertAgent åˆå§‹åŒ–å®Œæˆ")

            self.agents["milvus_expert"] = MilvusExpertAgent()
            self.output.print_success("âœ… MilvusExpertAgent åˆå§‹åŒ–å®Œæˆ")

            self.agents["devops_expert"] = DevOpsExpertAgent()
            self.output.print_success("âœ… DevOpsExpertAgent åˆå§‹åŒ–å®Œæˆ")

            # Test OpenAI client connectivity
            try:
                test_client = get_agent_config("coordination")
                client = OpenAIClientWrapper(config=test_client)
                # Simple test call
                test_response = client.get_chat_completion(
                    messages=[{"role": "user", "content": "test"}],
                    max_tokens=1
                )
                self.output.print_success("âœ… OpenAI API è¿æ¥æµ‹è¯•é€šè¿‡")
            except Exception as e:
                self.output.print_warning(f"âš ï¸  OpenAI API è¿æ¥æµ‹è¯•å¤±è´¥: {e}")

            # Test SharedMemory connectivity
            try:
                health = self.memory.health_check()
                if health.get("milvus_connected", False):
                    self.output.print_success("âœ… Milvus æ•°æ®åº“è¿æ¥æ­£å¸¸")
                else:
                    self.output.print_warning("âš ï¸  Milvus æ•°æ®åº“è¿æ¥å¼‚å¸¸")
            except Exception as e:
                self.output.print_warning(f"âš ï¸  SharedMemory å¥åº·æ£€æŸ¥å¤±è´¥: {e}")

            return self.agents

        except Exception as e:
            self.output.print_error(f"âŒ æ™ºèƒ½ä½“åˆå§‹åŒ–å¤±è´¥: {e}")
            raise

    async def process_question(self, question: str, tenant_id: str = "demo") -> Dict[str, Any]:
        """Process a single question through the multi-agent system."""
        self.question_count += 1
        question_id = f"q_{self.question_count:03d}"
        
        start_time = time.time()
        
        self.output.print_question(question_id, question)
        
        try:
            # Prepare message for coordinator
            message = {
                "content": {"text": question},
                "id": question_id,
                "tenant_id": tenant_id,
                "timestamp": datetime.now().isoformat()
            }

            # Process through coordinator
            coordinator = self.agents["coordinator"]
            response = await coordinator.handle_message(message)
            
            processing_time = time.time() - start_time
            
            # Extract metadata and results
            metadata = response.metadata or {}
            
            result = {
                "question_id": question_id,
                "question": question,
                "answer": response.content,
                "processing_time": processing_time,
                "metadata": metadata,
                "tenant_id": tenant_id,
                "timestamp": datetime.now().isoformat()
            }

            # Display results
            self.output.print_result(result)
            
            # Show knowledge base statistics
            if self.memory:
                try:
                    stats = self.memory.get_collection_stats("collaboration_history", tenant_id)
                    self.output.print_knowledge_stats(stats)
                except Exception as e:
                    self.output.print_warning(f"âš ï¸  æ— æ³•è·å–çŸ¥è¯†åº“ç»Ÿè®¡: {e}")

            return result

        except Exception as e:
            processing_time = time.time() - start_time
            error_result = {
                "question_id": question_id,
                "question": question,
                "answer": None,
                "error": str(e),
                "processing_time": processing_time,
                "tenant_id": tenant_id,
                "timestamp": datetime.now().isoformat()
            }
            
            self.output.print_error_result(error_result)
            logger.exception(f"Error processing question {question_id}")
            return error_result

    async def run_interactive_mode(self):
        """Run the DEMO in interactive mode."""
        self.output.print_section("ğŸ¤– Multi-Agent Brain äº¤äº’å¼ DEMO")
        self.output.print_info("è¾“å…¥é—®é¢˜è¿›è¡Œæµ‹è¯•ï¼ˆè¾“å…¥ 'quit', 'exit', æˆ– 'q' é€€å‡ºï¼‰")
        self.output.print_info("è¾“å…¥ 'help' æŸ¥çœ‹å¯ç”¨å‘½ä»¤")
        
        while True:
            try:
                question = input("\nğŸ“ æ‚¨çš„é—®é¢˜: ").strip()
                
                if question.lower() in ["quit", "exit", "q"]:
                    self.output.print_info("\nğŸ‘‹ å†è§ï¼")
                    break
                
                if question.lower() == "help":
                    self._show_help()
                    continue
                
                if not question:
                    continue
                
                await self.process_question(question)
                
            except KeyboardInterrupt:
                self.output.print_info("\n\nä¸­æ–­æ¼”ç¤º")
                break
            except EOFError:
                self.output.print_info("\n\nå†è§ï¼")
                break

    def _show_help(self):
        """Show help information for interactive mode."""
        help_text = """
ğŸ“‹ å¯ç”¨å‘½ä»¤:
  help     - æ˜¾ç¤ºæ­¤å¸®åŠ©ä¿¡æ¯
  quit     - é€€å‡º DEMO
  exit     - é€€å‡º DEMO  
  q        - é€€å‡º DEMO

ğŸ’¡ ç¤ºä¾‹é—®é¢˜:
  - å¦‚ä½•ç”¨ Python ä¼˜åŒ–åˆ—è¡¨æ¨å¯¼å¼çš„æ€§èƒ½ï¼Ÿ
  - Milvus å‘é‡æ•°æ®åº“å¦‚ä½•å¤„ç†é«˜ç»´å‘é‡æœç´¢ï¼Ÿ
  - å¦‚ä½•åœ¨ Docker ä¸­éƒ¨ç½² multi-agent-brain ç³»ç»Ÿï¼Ÿ
  - ä½¿ç”¨ Python å’Œ Milvus æ„å»ºå®æ—¶å‘é‡æœç´¢ç³»ç»Ÿçš„æœ€ä½³å®è·µæ˜¯ä»€ä¹ˆï¼Ÿ
  - å¦‚ä½•ç›‘æ§å’Œä¼˜åŒ– multi-agent ç³»ç»Ÿçš„æ€§èƒ½ï¼Ÿ
        """
        self.output.print_info(help_text)

    async def run_automated_mode(self):
        """Run the DEMO in automated mode with predefined questions."""
        self.output.print_section("ğŸ¤– Multi-Agent Brain è‡ªåŠ¨åŒ– DEMO")
        
        # Load predefined questions
        questions_file = project_root / "demo_questions.json"
        if not questions_file.exists():
            self.output.print_error(f"âŒ é—®é¢˜æ–‡ä»¶ä¸å­˜åœ¨: {questions_file}")
            return
        
        try:
            with open(questions_file, 'r', encoding='utf-8') as f:
                questions_data = json.load(f)
            
            questions = questions_data.get("questions", [])
            if not questions:
                self.output.print_warning("âš ï¸  æ²¡æœ‰æ‰¾åˆ°é¢„å®šä¹‰é—®é¢˜")
                return
            
            self.output.print_info(f"ğŸ“‹ å¼€å§‹å¤„ç† {len(questions)} ä¸ªé¢„å®šä¹‰é—®é¢˜...")
            
            results = []
            for i, q_data in enumerate(questions, 1):
                question = q_data.get("question", "")
                category = q_data.get("category", "unknown")
                expected_experts = q_data.get("expected_expert", [])
                
                self.output.print_info(f"\n[{i}/{len(questions)}] ç±»åˆ«: {category} | é¢„æœŸä¸“å®¶: {', '.join(expected_experts)}")
                
                result = await self.process_question(question, tenant_id="automated_demo")
                result["category"] = category
                result["expected_experts"] = expected_experts
                results.append(result)
                
                # Small delay between questions
                await asyncio.sleep(1)
            
            # Show summary
            self._show_automated_summary(results)
            
        except Exception as e:
            self.output.print_error(f"âŒ è‡ªåŠ¨åŒ–æ¨¡å¼æ‰§è¡Œå¤±è´¥: {e}")
            logger.exception("Automated mode failed")

    def _show_automated_summary(self, results: List[Dict[str, Any]]):
        """Show summary of automated test results."""
        self.output.print_section("ğŸ“Š è‡ªåŠ¨åŒ–æµ‹è¯•æ€»ç»“")
        
        total = len(results)
        successful = sum(1 for r in results if r.get("answer") and not r.get("error"))
        failed = total - successful
        
        avg_time = sum(r.get("processing_time", 0) for r in results) / total if total > 0 else 0
        
        summary = f"""
æ€»é—®é¢˜æ•°: {total}
æˆåŠŸå¤„ç†: {successful}
å¤„ç†å¤±è´¥: {failed}
å¹³å‡å¤„ç†æ—¶é—´: {avg_time:.2f} ç§’
æˆåŠŸç‡: {(successful/total*100):.1f}%
        """
        self.output.print_info(summary)
        
        # Show category breakdown
        categories = {}
        for result in results:
            cat = result.get("category", "unknown")
            if cat not in categories:
                categories[cat] = {"total": 0, "success": 0}
            categories[cat]["total"] += 1
            if result.get("answer") and not result.get("error"):
                categories[cat]["success"] += 1
        
        if categories:
            self.output.print_info("\nğŸ“ˆ åˆ†ç±»ç»Ÿè®¡:")
            for cat, stats in categories.items():
                success_rate = (stats["success"] / stats["total"] * 100) if stats["total"] > 0 else 0
                self.output.print_info(f"  {cat}: {stats['success']}/{stats['total']} ({success_rate:.1f}%)")

    async def run_benchmark_mode(self):
        """Run the DEMO in benchmark mode for performance testing."""
        self.output.print_section("ğŸš€ Multi-Agent Brain æ€§èƒ½åŸºå‡†æµ‹è¯•")
        
        # Define benchmark questions
        benchmark_questions = [
            "å¦‚ä½•ä¼˜åŒ– Python ä»£ç æ€§èƒ½ï¼Ÿ",
            "Milvus æ•°æ®åº“çš„æœ€ä½³å®è·µæ˜¯ä»€ä¹ˆï¼Ÿ",
            "å¦‚ä½•éƒ¨ç½²å®¹å™¨åŒ–åº”ç”¨ï¼Ÿ",
            "å‘é‡æœç´¢çš„æ€§èƒ½ä¼˜åŒ–æŠ€å·§ï¼Ÿ",
            "å¤šçº¿ç¨‹ç¼–ç¨‹çš„æœ€ä½³å®è·µï¼Ÿ"
        ]
        
        concurrent_levels = [1, 3, 5]  # Different concurrency levels
        
        for concurrency in concurrent_levels:
            self.output.print_info(f"\nğŸ”„ æµ‹è¯•å¹¶å‘çº§åˆ«: {concurrency}")
            
            start_time = time.time()
            tasks = []
            
            for i in range(concurrency):
                question = benchmark_questions[i % len(benchmark_questions)]
                task = self.process_question(
                    question, 
                    tenant_id=f"benchmark_concurrency_{concurrency}"
                )
                tasks.append(task)
            
            results = await asyncio.gather(*tasks, return_exceptions=True)
            total_time = time.time() - start_time
            
            # Calculate metrics
            successful = sum(1 for r in results if not isinstance(r, Exception) and r.get("answer"))
            avg_response_time = sum(r.get("processing_time", 0) for r in results if not isinstance(r, Exception)) / len(results)
            
            throughput = successful / total_time if total_time > 0 else 0
            
            metrics = f"""
å¹¶å‘çº§åˆ«: {concurrency}
æ€»è€—æ—¶: {total_time:.2f} ç§’
æˆåŠŸè¯·æ±‚: {successful}/{len(results)}
å¹³å‡å“åº”æ—¶é—´: {avg_response_time:.2f} ç§’
ååé‡: {throughput:.2f} è¯·æ±‚/ç§’
            """
            self.output.print_info(metrics)
            
            # Small delay between concurrency levels
            await asyncio.sleep(2)

    async def run(self):
        """Main entry point for running the DEMO."""
        try:
            # Check environment first
            if not check_environment():
                self.output.print_error("âŒ ç¯å¢ƒæ£€æŸ¥å¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®")
                return
            
            # Setup agents
            await self.setup_agents()
            
            # Run based on mode
            if self.mode == DemoMode.INTERACTIVE:
                await self.run_interactive_mode()
            elif self.mode == DemoMode.AUTOMATED:
                await self.run_automated_mode()
            elif self.mode == DemoMode.BENCHMARK:
                await self.run_benchmark_mode()
            else:
                self.output.print_error(f"âŒ ä¸æ”¯æŒçš„æ¨¡å¼: {self.mode}")
                
        except KeyboardInterrupt:
            self.output.print_info("\n\næ¼”ç¤ºè¢«ç”¨æˆ·ä¸­æ–­")
        except Exception as e:
            self.output.print_error(f"âŒ DEMO æ‰§è¡Œå¤±è´¥: {e}")
            logger.exception("DEMO execution failed")
        finally:
            self.output.print_section("ğŸ DEMO ç»“æŸ")


@click.command()
@click.option('--mode', '-m', 
              type=click.Choice(['interactive', 'automated', 'benchmark'], case_sensitive=False),
              default='interactive', 
              help='DEMO è¿è¡Œæ¨¡å¼')
@click.option('--config', '-c', 
              default='config.yaml', 
              help='é…ç½®æ–‡ä»¶è·¯å¾„')
def main(mode: str, config: str):
    """å¯åŠ¨ Multi-Agent Brain DEMO
    
    MODES:
      interactive  äº¤äº’å¼æ¨¡å¼ï¼Œç”¨æˆ·æ‰‹åŠ¨è¾“å…¥é—®é¢˜
      automated    è‡ªåŠ¨åŒ–æ¨¡å¼ï¼Œä½¿ç”¨é¢„å®šä¹‰é—®é¢˜é›†
      benchmark    æ€§èƒ½æµ‹è¯•æ¨¡å¼ï¼Œæµ‹è¯•å¹¶å‘æ€§èƒ½
    
    EXAMPLES:
      python demo_runner.py                          # äº¤äº’å¼æ¨¡å¼
      python demo_runner.py --mode automated         # è‡ªåŠ¨åŒ–æ¨¡å¼
      python demo_runner.py --mode benchmark         # æ€§èƒ½æµ‹è¯•
    """
    # Configure logging
    logger.remove()
    logger.add(
        sys.stderr,
        format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level: <8}</level> | <cyan>{extra[agent_id]}</cyan> | <level>{message}</level>",
        level="INFO"
    )
    
    # Run demo
    demo = MultiAgentDemo(mode=mode, config_file=config)
    asyncio.run(demo.run())


if __name__ == "__main__":
    main()